{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Title\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "A USA based tourist agency what’s to analyze USA visitors, which can be done using the I94 immigration data. the goal is to structure a warehouse server in order to answer queries about the visitors Behaviors. Queries such as the following:\n",
    "* How long visitors from a given county in average stay?\n",
    "* What time of year has the most visitors for tourism?\n",
    "* What state has the most visitors in given month? \n",
    "* What states does a given group of people more likely to visit?\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import boto3\n",
    "import io\n",
    "import configparser\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "\n",
    "The tourist agency is new, and it doesn't have the budget to gather the wanted data. So, it used I94 immigration data, which is an open data from the US National Tourism and Trade Office. This data might not answer all of their future business intelligence questions but it’s a good start. The scope of this project is to make a warehouse ready to analyze the behavior of USA state tourists that are captured in the i94 immigration data. \n",
    "\n",
    "In order to analyze the tourist’s behavior in groups, a USA demographic data is included. the demographic data is from the US Census Bureau's 2015 American Community Survey which is available in website called Opensoft. \n",
    "\n",
    "the agency wanted to make this warehouse and analyze the data as soon as possible, using familiar tools such as SQL. because of that, the database selected is a relational database management system (RDMS) which allows for fast aggregation using familiar tools. Of course, warehouse is Online Analytical Processing (OLAP) database that will help the agency make intelligent business decisions.\n",
    "\n",
    "For easy setup the warehouse is made using a cloud service. cloud services allow for easy expansion and adjacent, which is a fit for the new agency. Amazon web services, offers a cloud RDMS solution called Redshift that is suitable of handling large datasets because of its parallel processing.\n",
    "\n",
    "\n",
    "\n",
    "#### Describe and Gather Data \n",
    "##### I94 immigration dataset\n",
    "The dataset comes from the US National Tourism and Trade Office. i94 is a form filled by non-USA citizens , to keep track of the USA vistors. The dataset includes 2016 data which good enough for the goal of the agency.\n",
    "##### USA Cities demographics data\n",
    "The demographic data is from the US Census Bureau's 2015 American Community Survey which is available in website called Opensoft. the data is gathered in 2015, which is close to the i94 data. this data shows all USA cities with population greater or equal than 65000. as mentioned before this project is focused on states not cities, due to the limitations of the i94 data. because of that the demographics data needs to be grouped up by states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 28)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Read in the data here from localy stored sas files.\n",
    "file_name = \"i94_jan16_sub\"\n",
    "fname = '../../data/18-83510-I94-Data-2016/'+file_name+'.sas7bdat'\n",
    "data_immig_chunks = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\", chunksize = 10000)\n",
    "df_chunk = next(data_immig_chunks)\n",
    "display(df_chunk.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cicid</th>\n",
       "      <th>i94yr</th>\n",
       "      <th>i94mon</th>\n",
       "      <th>i94cit</th>\n",
       "      <th>i94res</th>\n",
       "      <th>i94port</th>\n",
       "      <th>arrdate</th>\n",
       "      <th>i94mode</th>\n",
       "      <th>i94addr</th>\n",
       "      <th>depdate</th>\n",
       "      <th>...</th>\n",
       "      <th>entdepu</th>\n",
       "      <th>matflag</th>\n",
       "      <th>biryear</th>\n",
       "      <th>dtaddto</th>\n",
       "      <th>gender</th>\n",
       "      <th>insnum</th>\n",
       "      <th>airline</th>\n",
       "      <th>admnum</th>\n",
       "      <th>fltno</th>\n",
       "      <th>visatype</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>346608285.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20465.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>MA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1996.0</td>\n",
       "      <td>D/S</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>LH</td>\n",
       "      <td>346627585.0</td>\n",
       "      <td>424</td>\n",
       "      <td>F1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20480.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AF</td>\n",
       "      <td>381092385.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>1971.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AF</td>\n",
       "      <td>381087885.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11.0</td>\n",
       "      <td>2016.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>BOS</td>\n",
       "      <td>20469.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>20499.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>2004.0</td>\n",
       "      <td>07152016</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>AF</td>\n",
       "      <td>381078685.0</td>\n",
       "      <td>338</td>\n",
       "      <td>B2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   cicid   i94yr  i94mon  i94cit  i94res i94port  arrdate  i94mode i94addr  \\\n",
       "0    7.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "1    8.0  2016.0     1.0   101.0   101.0     BOS  20465.0      1.0      MA   \n",
       "2    9.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "3   10.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "4   11.0  2016.0     1.0   101.0   101.0     BOS  20469.0      1.0      CT   \n",
       "\n",
       "   depdate   ...     entdepu  matflag  biryear   dtaddto gender insnum  \\\n",
       "0      NaN   ...         NaN      NaN   1996.0       D/S      M    NaN   \n",
       "1      NaN   ...         NaN      NaN   1996.0       D/S      M    NaN   \n",
       "2  20480.0   ...         NaN        M   1999.0  07152016      F    NaN   \n",
       "3  20499.0   ...         NaN        M   1971.0  07152016      F    NaN   \n",
       "4  20499.0   ...         NaN        M   2004.0  07152016      M    NaN   \n",
       "\n",
       "  airline       admnum fltno visatype  \n",
       "0      LH  346608285.0   424       F1  \n",
       "1      LH  346627585.0   424       F1  \n",
       "2      AF  381092385.0   338       B2  \n",
       "3      AF  381087885.0   338       B2  \n",
       "4      AF  381078685.0   338       B2  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chunk.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>Median Age</th>\n",
       "      <th>Male Population</th>\n",
       "      <th>Female Population</th>\n",
       "      <th>Total Population</th>\n",
       "      <th>Number of Veterans</th>\n",
       "      <th>Foreign-born</th>\n",
       "      <th>Average Household Size</th>\n",
       "      <th>State Code</th>\n",
       "      <th>Race</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silver Spring</td>\n",
       "      <td>Maryland</td>\n",
       "      <td>33.8</td>\n",
       "      <td>40601.0</td>\n",
       "      <td>41862.0</td>\n",
       "      <td>82463</td>\n",
       "      <td>1562.0</td>\n",
       "      <td>30908.0</td>\n",
       "      <td>2.60</td>\n",
       "      <td>MD</td>\n",
       "      <td>Hispanic or Latino</td>\n",
       "      <td>25924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Quincy</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>41.0</td>\n",
       "      <td>44129.0</td>\n",
       "      <td>49500.0</td>\n",
       "      <td>93629</td>\n",
       "      <td>4147.0</td>\n",
       "      <td>32935.0</td>\n",
       "      <td>2.39</td>\n",
       "      <td>MA</td>\n",
       "      <td>White</td>\n",
       "      <td>58723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hoover</td>\n",
       "      <td>Alabama</td>\n",
       "      <td>38.5</td>\n",
       "      <td>38040.0</td>\n",
       "      <td>46799.0</td>\n",
       "      <td>84839</td>\n",
       "      <td>4819.0</td>\n",
       "      <td>8229.0</td>\n",
       "      <td>2.58</td>\n",
       "      <td>AL</td>\n",
       "      <td>Asian</td>\n",
       "      <td>4759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rancho Cucamonga</td>\n",
       "      <td>California</td>\n",
       "      <td>34.5</td>\n",
       "      <td>88127.0</td>\n",
       "      <td>87105.0</td>\n",
       "      <td>175232</td>\n",
       "      <td>5821.0</td>\n",
       "      <td>33878.0</td>\n",
       "      <td>3.18</td>\n",
       "      <td>CA</td>\n",
       "      <td>Black or African-American</td>\n",
       "      <td>24437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Newark</td>\n",
       "      <td>New Jersey</td>\n",
       "      <td>34.6</td>\n",
       "      <td>138040.0</td>\n",
       "      <td>143873.0</td>\n",
       "      <td>281913</td>\n",
       "      <td>5829.0</td>\n",
       "      <td>86253.0</td>\n",
       "      <td>2.73</td>\n",
       "      <td>NJ</td>\n",
       "      <td>White</td>\n",
       "      <td>76402</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               City          State  Median Age  Male Population  \\\n",
       "0     Silver Spring       Maryland        33.8          40601.0   \n",
       "1            Quincy  Massachusetts        41.0          44129.0   \n",
       "2            Hoover        Alabama        38.5          38040.0   \n",
       "3  Rancho Cucamonga     California        34.5          88127.0   \n",
       "4            Newark     New Jersey        34.6         138040.0   \n",
       "\n",
       "   Female Population  Total Population  Number of Veterans  Foreign-born  \\\n",
       "0            41862.0             82463              1562.0       30908.0   \n",
       "1            49500.0             93629              4147.0       32935.0   \n",
       "2            46799.0             84839              4819.0        8229.0   \n",
       "3            87105.0            175232              5821.0       33878.0   \n",
       "4           143873.0            281913              5829.0       86253.0   \n",
       "\n",
       "   Average Household Size State Code                       Race  Count  \n",
       "0                    2.60         MD         Hispanic or Latino  25924  \n",
       "1                    2.39         MA                      White  58723  \n",
       "2                    2.58         AL                      Asian   4759  \n",
       "3                    3.18         CA  Black or African-American  24437  \n",
       "4                    2.73         NJ                      White  76402  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "USA_cities_df = pd.read_csv(\"us-cities-demographics.csv\", sep=';', error_bad_lines=False)\n",
    "USA_cities_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "pandas library is used to explore the i94 immigration dataset. Because the dataset is in a table form pandas library is an intuitive tool for this goal. Also because the i94 immigration data is in SAS format, and the database is using redshift the files is transformed to csv using pandas library. The steps of transforming data to csv can be seen below can be seen below. The csv files are then uploaded into an S3 server. this take time, but only needs to be done once.\n",
    "\n",
    "The i94 data has a lot of data that is not explained even with the SAS description file. So, the goal is clear out these data, and focus on what is useful. Because of that many columns in the staging table is not used. the i94 data includes three types of visitors based on the intended activity which is stored in column I94VISA in the staging table. since the database is for a travel agency they are only interested to study the activities of the visitors that visit the USA for pleasure motives. \n",
    "#### Cleaning Steps\n",
    "\n",
    "Before transforming the data, frame chunks the data undergo a cleaning process as seen below. a problem that is created from the transformation from cvs to that Null values in columns type VARCHAR are copied as empty strings. So, when the data is in the staging table the empty strings is transformed back to null values. After that, more cleaning is done in SQL. the ETL process used is a staging then insert process, so the cleaning is done using the insert queries. see the Step 4 section to see the process.\n",
    "\n",
    "### <span style=\"color:red\"> DO NOT RUN THE BELOW CELLS UNLESS YOU WANT TO UPLOAD THE LOACAL SAS FILES TO S3 </span>.\n",
    "\n",
    "*note: S3 file structure is \"../month/file_part#.csv\"*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat\n"
     ]
    }
   ],
   "source": [
    "#To reduce the files numbers , the chunksize has been increased.\n",
    "#change the file_month to change files\n",
    "file_month = 'apr'\n",
    "file_name = \"i94_{}16_sub\".format(file_month)\n",
    "fname = '../../data/18-83510-I94-Data-2016/{}.sas7bdat'.format(file_name)\n",
    "data_immig_chunks = pd.read_sas(fname, 'sas7bdat', encoding=\"ISO-8859-1\", chunksize = 1000000)\n",
    "print(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#This dictionry is created to translate the contant of i94cit, and i94res \n",
    "#translate it from codes to strings\n",
    "country_codes = pd.read_csv(\"country_codes.csv\", sep=r',', error_bad_lines=False)\n",
    "country_codes_dict = {}\n",
    "\n",
    "for index, row in country_codes.iterrows():\n",
    "    country_codes_dict[row['code']] = row['country']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get info from dwh.cfg file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "KEY = config.get('AWS','KEY')\n",
    "SECRET = config.get('AWS','SECRET')\n",
    "\n",
    "file_key= config.get('S3','file_key')\n",
    "bucket = config.get('S3','bucket')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Initialize the s3 server to upload in\n",
    "s3_resource = boto3.resource('s3',\n",
    "                       aws_access_key_id=KEY,\n",
    "                       aws_secret_access_key=SECRET\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dimensions of chunk before cleaning: (1000000, 28)\n",
      "dimensions of chunk after cleaning: (928367, 28)\n",
      "file :i94_apr16_sub_part1.csv Uploaded!\n",
      "dimensions of chunk before cleaning: (1000000, 28)\n",
      "dimensions of chunk after cleaning: (888034, 28)\n",
      "file :i94_apr16_sub_part2.csv Uploaded!\n",
      "dimensions of chunk before cleaning: (96313, 28)\n",
      "dimensions of chunk after cleaning: (65169, 28)\n",
      "file :i94_apr16_sub_part3.csv Uploaded!\n"
     ]
    }
   ],
   "source": [
    "#Convert dataframes into CSV files and upload them to S3\n",
    "count = 1\n",
    "for df_chunk in data_immig_chunks:\n",
    "    #clearing out None values fro important columns\n",
    "    print(\"dimensions of chunk before cleaning: \"+str(df_chunk.shape))\n",
    "    df_chunk.dropna(subset=['cicid', 'i94cit', 'i94res', 'i94mode',\\\n",
    "                            'fltno', 'arrdate', 'depdate', 'i94addr'],inplace=True)\n",
    "    print(\"dimensions of chunk after cleaning: \"+str(df_chunk.shape))\n",
    "    \n",
    "    #translate the codes of i94mode.\n",
    "    df_chunk = df_chunk.replace({'i94mode':{ 1:'Air', 2 : 'Sea', 3:'Land'}})\n",
    "    #translate the codes of i94cit, and i94res.\n",
    "    df_chunk = df_chunk.replace({'i94cit': country_codes_dict})\n",
    "    df_chunk = df_chunk.replace({'i94res': country_codes_dict})\n",
    "    \n",
    "    #transform dataframe to csv \n",
    "    csv_buffer = io.StringIO()\n",
    "    df_chunk.to_csv(csv_buffer, index=False)\n",
    "    \n",
    "    #upload file to s3\n",
    "    s3_resource.Object(bucket,'{}/{}/{}_part{}.csv'.format(file_key,file_month,file_name,count))\\\n",
    "        .put(Body=csv_buffer.getvalue())\n",
    "    print(\"file :\"+'{}_part{}.csv'.format(file_name,count)+\" Uploaded!\")\n",
    "    count = count+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Define the Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "The model used is a star schema data model. the schema is selected because, it makes queries simple, and aggregation fast. As can be seen in the graph below, the fact table is focused on dates, which is what the i94 form is forced on too. The dimension tables are for the tourists, flights, and USA states.\n",
    "\n",
    "![Data Model](USA_Tourism_data_model_v1.png)\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "Once the needed data in csv form in s3, the model can be implemented with following steps: \n",
    "\n",
    "1. Copy data to from S3 to Redshift.\n",
    "2. Converting empty strings to null\n",
    "2. Insert and transform the data into the model tables. \n",
    "\n",
    "Copy and insert queries are from file ```sql_queries.py```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "To create the data model, first tables needs to be made in the Redshift server, assuming that Redshift server is already up and running. This can be done by ``` run create_table.py``` in a python console. After that the staging query for the i94 data, needs to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sql_queries import insert_table_queries, copy_table_queries,\\\n",
    "states_table_insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get info from dwh.cfg file\n",
    "config = configparser.ConfigParser()\n",
    "config.read('dwh.cfg')\n",
    "\n",
    "#connect to the Redshift server\n",
    "conn = psycopg2.connect(\"host={} dbname={} user={} password={} port={}\".\\\n",
    "                        format(*config['CLUSTER'].values()))\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:  copy staging_i94 \n",
      "    FROM 's3://dend-bucket-azizdul/i94_csv_files/apr/'\n",
      "    credentials 'aws_iam_role=arn:aws:iam::397229940073:role/dwhRole'\n",
      "\tdelimiter ',' removequotes\n",
      "\tIGNOREHEADER 1\n",
      "    region 'us-east-2' ;\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "#Copy data from S3 to Staging table \n",
    "print(\"Query: \"+copy_table_queries)\n",
    "cur.execute(copy_table_queries)\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1881570,)]\n"
     ]
    }
   ],
   "source": [
    "#showing that null values apper as a empty strings \n",
    "cur.execute(\"SELECT count(*) FROM staging_i94 WHERE '' IN (i94port, visapost, gender, airline,visatype, occup,entdepa, entdepd,entdepu,matflag,dtaddto, insnum)\")\n",
    "conn.commit()\n",
    "query_answer = cur.fetchall()\n",
    "print(query_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting empty strings to null\n",
      "Update staging_i94 set i94port = NULL where i94port = ''\n",
      "Update staging_i94 set visapost = NULL where visapost = ''\n",
      "Update staging_i94 set gender = NULL where gender = ''\n",
      "Update staging_i94 set airline = NULL where airline = ''\n",
      "Update staging_i94 set visatype = NULL where visatype = ''\n",
      "Update staging_i94 set occup = NULL where occup = ''\n",
      "Update staging_i94 set entdepa = NULL where entdepa = ''\n",
      "Update staging_i94 set entdepd = NULL where entdepd = ''\n",
      "Update staging_i94 set entdepu = NULL where entdepu = ''\n",
      "Update staging_i94 set matflag = NULL where matflag = ''\n",
      "Update staging_i94 set dtaddto = NULL where dtaddto = ''\n",
      "Update staging_i94 set insnum = NULL where insnum = ''\n",
      "DONE!\n"
     ]
    }
   ],
   "source": [
    "#list of columns of type VARCHAR\n",
    "columns_list = ['i94port', 'visapost', 'gender', 'airline','visatype', 'occup','entdepa', 'entdepd','entdepu','matflag','dtaddto', 'insnum' ]\n",
    "print(\"converting empty strings to null\")\n",
    "for column in columns_list:\n",
    "    try:\n",
    "        print(\"Update staging_i94 set {} = NULL where {} = ''\".format(column,column))\n",
    "        cur.execute(\"Update staging_i94 set {} = NULL where {} = ''\".format(column,column))\n",
    "        conn.commit()\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error: Inserting Rows\")\n",
    "        print (e)\n",
    "print('DONE!')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,)]\n"
     ]
    }
   ],
   "source": [
    "#conforming that null values are turned back to null\n",
    "cur.execute(\"SELECT count(*) FROM staging_i94 WHERE '' IN (i94port, visapost, gender, airline,visatype, occup,entdepa, entdepd,entdepu,matflag,dtaddto, insnum)\")\n",
    "conn.commit()\n",
    "query_answer = cur.fetchall()\n",
    "print(query_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: \n",
      "    INSERT INTO tourists( \n",
      "    cicid, i94cit, i94res, \n",
      "    i94mode,visapost, biryear, gender, visatype)\n",
      "        SELECT DISTINCT \n",
      "        cicid, i94cit, i94res, \n",
      "        i94mode, visapost, biryear,\n",
      "        gender, visatype\n",
      "        FROM staging_i94\n",
      "        WHERE cicid IS NOT NULL\n",
      "        AND i94mode IS NOT NULL\n",
      "        AND i94visa = 2.0\n",
      "        AND cicid NOT IN (SELECT cicid \n",
      "                         from tourists)\n",
      "\n",
      "Query: \n",
      "    INSERT INTO flights \n",
      "    (fltno, airline, i94port)\n",
      "        SELECT DISTINCT\n",
      "        fltno, airline, i94port\n",
      "        FROM staging_i94\n",
      "        WHERE fltno IS NOT NULL\n",
      "        AND fltno != 'LAND' AND fltno != 'SEA'\n",
      "        AND i94visa = 2.0\n",
      "        AND fltno NOT IN (SELECT fltno \n",
      "                         from flights)\n",
      "\n",
      "Query: \n",
      "    INSERT INTO stay_durations \n",
      "    (cicid, fltno,arr_state_code, arrdate, depdate)\n",
      "        SELECT DISTINCT\n",
      "        cicid, fltno, i94addr,\n",
      "        DATEADD(day, CAST(arrdate AS int) ,'1960-1-1'),\n",
      "        DATEADD(day, CAST(depdate AS int) ,'1960-1-1')\n",
      "        FROM staging_i94\n",
      "        WHERE fltno IS NOT NULL \n",
      "        AND cicid IS NOT NULL\n",
      "        AND i94mode IS NOT NULL\n",
      "        AND i94visa = 2.0\n",
      "        AND i94addr IS NOT NULL\n",
      "        AND depdate IS NOT NULL\n",
      "        AND cicid NOT IN (SELECT cicid \n",
      "                         from stay_durations)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#execute insert queries from staging table\n",
    "for query in insert_table_queries:\n",
    "    print(\"Query: \"+query)\n",
    "    try:\n",
    "        cur.execute(query)\n",
    "        conn.commit()\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error: Inserting Rows\")\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#group demographics data by state\n",
    "USA_state_df = USA_cities_df.groupby(by=[\"State Code\",\"State\"],  as_index=False).agg({\n",
    "                                                    'Male Population': 'sum',\n",
    "                                                    'Female Population': 'sum',\n",
    "                                                    'Total Population': 'sum',\n",
    "                                                    'Foreign-born': 'sum',\n",
    "                                                    'Median Age': 'median',\n",
    "                                                    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#get values from Dataframe\n",
    "USA_states_data = USA_state_df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#execute insert query for states table\n",
    "USA_states_data\n",
    "for row in USA_states_data:\n",
    "    try:\n",
    "        cur.execute(states_table_insert, row)\n",
    "        conn.commit()\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error: Inserting Rows\")\n",
    "        print (e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks\n",
    "Explain the data quality checks you'll perform to ensure the pipeline ran as expected. These could include:\n",
    " * Integrity constraints on the relational database (e.g., unique key, data type, etc.)\n",
    " * Unit tests for the scripts to ensure they are doing the right thing\n",
    " * Source/Count checks to ensure completeness\n",
    " \n",
    "Run Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if tables in DB contain data\n",
      "tourists: Pass\n",
      "flights: Pass\n",
      "states: Pass\n",
      "stay_durations: Pass\n"
     ]
    }
   ],
   "source": [
    "# Quality check if tables are empty\n",
    "tables_list = ['tourists', 'flights', 'states', 'stay_durations']\n",
    "print(\"Checking if tables in DB contain data\")\n",
    "for table in tables_list:\n",
    "    try:\n",
    "        cur.execute(\"select count(*) from {}\".format(table))\n",
    "        conn.commit()\n",
    "        query_answer = cur.fetchall()\n",
    "        if(query_answer[0][0]> 0):\n",
    "            print(table+\": Pass\")\n",
    "        else:\n",
    "            print(table+\": Fail\")\n",
    "    except psycopg2.Error as e:\n",
    "        print(\"Error: Inserting Rows\")\n",
    "        print (e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking if tables in DB contain data\n",
      "SELECT cicid from tourists WHERE gender IS NULL\n",
      "Null test for tourists table with column gender: Fail\n",
      "SELECT cicid from stay_durations WHERE fltno IS NULL\n",
      "Null test for stay_durations table with column fltno: Pass\n"
     ]
    }
   ],
   "source": [
    "#check if columns have null rows\n",
    "#put in checking_list (table,primery key, column)\n",
    "checking_list = [['tourists', 'cicid', 'gender'],['stay_durations', 'cicid', 'fltno'] ]\n",
    "print(\"Checking if tables in DB contain data\")\n",
    "for n in checking_list:\n",
    "    print(\"SELECT {} from {} WHERE {} IS NULL\".format(n[1],n[0],n[2]))\n",
    "    cur.execute(\"SELECT {} from {} WHERE {} IS NULL\".format(n[1],n[0],n[2]))\n",
    "    query_answer = cur.fetchall()\n",
    "    #print(query_answer)\n",
    "    if not query_answer:\n",
    "        print(\"Null test for {} table with column {}\".format(n[0], n[2])+\": Pass\")\n",
    "    else:\n",
    "        print(\"Null test for {} table with column {}\".format(n[0], n[2])+\": Fail\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "| Field              | Type    | Table          | Explanation                                                                       | Source                 |\n",
    "| ------------------ | ------- | -------------- | --------------------------------------------------------------------------------- | ---------------------- |\n",
    "| i94port            | VARCHAR | Flights        | This is the code of the airport the visitors landed on                            | I94 Immigration        |\n",
    "| airline            | VARCHAR | Flights        | The Airline compony used by the visitor                                           | I94 Immigration        |\n",
    "| fltno              | VARCHAR | Flights        | The flight number the visitor arrived with                                        | I94 Immigration        |\n",
    "| arr\\_state\\_code   | VARCHAR | Stay\\_duration |  The state abbreviations/code that the visitor arrived on                         | I94 Immigration        |\n",
    "| arrdate            | NUMERIC | Stay\\_duration | the date the visitor arrived                                                      | I94 Immigration        |\n",
    "| depdate            | NUMERIC | Stay\\_duration | the date the visitor departed                                                     | I94 Immigration        |\n",
    "| cicid              | FLOAT   | Tourists       | Id for each visitor                                                               | I94 Immigration        |\n",
    "| i94cit             | NUMERIC | Tourists       | the city the visitor arrived from                                                 | I94 Immigration        |\n",
    "| 94res              | NUMERIC | Tourists       | the city the visitor resides in                                                   | I94 Immigration        |\n",
    "| i94mode            | VARCHAR | Tourists       | the transportation method that the visitor entered the USA with                   | I94 Immigration        |\n",
    "| visapost           | VARCHAR | Tourists       | Department of State where  the visitor Visa was issued                            | I94 Immigration        |\n",
    "| biryear            | NUMERIC | Tourists       | birth year of the visitor                                                         | I94 Immigration        |\n",
    "| gender             | VARCHAR | Tourists       | the gender of the visitor                                                         | I94 Immigration        |\n",
    "| visatype           | VARCHAR | Tourists       | Class of admission legally admitting the non-immigrant to temporarily stay in U.S | I94 Immigration        |\n",
    "| state\\_code        | VARCHAR | States         |  The state abbreviations/code                                                     | us cities demographics |\n",
    "| state              | VARCHAR | States         |  The state Name                                                                   | us cities demographics |\n",
    "| female\\_pop        | NUMERIC | States         | Total number of female population                                                 | us cities demographics |\n",
    "| total\\_pop         | NUMERIC | States         | Total number of male population                                                   | us cities demographics |\n",
    "| foreign\\_born\\_num | NUMERIC | States         | Total number of foreign born population                                           | us cities demographics |\n",
    "| median\\_Age        | NUMERIC | States         | the age median of the state                                                       | us cities demographics |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.4 Q&A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Clearly state the rationale for the choice of tools and technologies for the project.\n",
    "#### AWS redshift: \n",
    "I selected redshift service because it can process large data such as the i94 data. Then using SQL, the queries stated at the beginning can be executed. \n",
    "#### AWS S3: \n",
    "from experience uploading data and downloading it is fast and accessible. Also, integrating it with redshift works well. \n",
    "#### Pandas: \n",
    "Pandas library provides an intuitive way to explore and transform 2d structured data , as can be seen in the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Propose how often the data should be updated and why.\n",
    "It depends on how often the US National Tourism and Trade Office uploads new i94 data. Regardless, monthly updates would be the most fitting, because daily or weekly doesn't change outcome of the intended queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Write a description of how you would approach the problem differently under the following scenarios:\n",
    "#### The data was increased by 100x.\n",
    "I would change the way of putting csv files to the S3 server and make the process more automated (i.e use Airflow) and make the process run on a more capable machine. Also, Because there are a lot of large table movement, I would scale up the redshift server. Meaning that nodes are more capable machines with more vCPUs onboard. which is also means increasing the capacity size of the redshift server.\n",
    "#### The data populates a dashboard that must be updated on a daily basis by 7am every day.\n",
    "From my previous assumption that data is updated at least in a monthly basis. But with this scenario, then a workflow management platform like airflow is needed. I would create two Directed Acyclic Graphs (DAGs), one that automate handling transforming the data to S3. Then, another one that handling data from S3 to staging, to tables, to quality checks. the DAGs are set with a daily schedule interval. \n",
    "#### The database needed to be accessed by 100+ people.\n",
    "In this case the Redshift server needs to be scaled out, meaning increase the number of nodes. this would allow the cluster to handle more queries at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
